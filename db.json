{
    "models":{
        "gemma-7b":{
            "description":"Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone",
            "provider":"Google",
            "code":"from transformers import AutoTokenizer, AutoModelForCausalLM /n tokenizer = AutoTokenizer.from_pretrained('google/gemma-7b') /n model = AutoModelForCausalLM.from_pretrained('google/gemma-7b') /n input_text = 'Write me a poem about Machine Learning.' /n input_ids = tokenizer(input_text, return_tensors='pt') /n outputs = model.generate(**input_ids) /n print(tokenizer.decode(outputs[0]))",
            "uses":"Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. /n Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. /n Text Summarization: Generate concise summaries of a text corpus, research papers, or reports."
        },
        "SDXL-Lightning":{
            "description":"SDXL-Lightning is a lightning-fast text-to-image generation model. It can generate high-quality 1024px images in a few steps. For more information, please refer to our research paper: SDXL-Lightning: Progressive Adversarial Diffusion Distillation. We open-source the model as part of the research. /n Our models are distilled from stabilityai/stable-diffusion-xl-base-1.0. This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models. The generation quality of our 2-step, 4-step, and 8-step model is amazing. Our 1-step model is more experimental. /n We provide both full UNet and LoRA checkpoints. The full UNet models have the best quality while the LoRA models can be applied to other base models.",
            "provider":"ByteDance",
            "code":"import torch /n from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler /n from huggingface_hub import hf_hub_download /n from safetensors.torch import load_file /n base = 'stabilityai/stable-diffusion-xl-base-1.0' /n repo = 'ByteDance/SDXL-Lightning' /n ckpt = 'sdxl_lightning_4step_unet.safetensors' # Use the correct ckpt for your step setting! /n unet = UNet2DConditionModel.from_config(base, subfolder='unet').to('cuda', torch.float16) /n unet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device='cuda')) /n pipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant='fp16').to('cuda') /n pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing='trailing') /n pipe('A girl smiling', num_inference_steps=4, guidance_scale=0).images[0].save('output.png')",
            "uses":""
        },
        "Gemma-7b-it":{
            "description":"This model is built upon the WÃ¼rstchen architecture and its main difference to other models like Stable Diffusion is that it is working at a much smaller latent space. Why is this important? The smaller the latent space, the faster you can run inference and the cheaper the training becomes. How small is the latent space? Stable Diffusion uses a compression factor of 8, resulting in a 1024x1024 image being encoded to 128x128. Stable Cascade achieves a compression factor of 42, meaning that it is possible to encode a 1024x1024 image to 24x24, while maintaining crisp reconstructions. The text-conditional model is then trained in the highly compressed latent space. Previous versions of this architecture, achieved a 16x cost reduction over Stable Diffusion 1.5.",
            "provider":"StabilityAI",
            "code":"import torch /n from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline /n device = 'cuda' /n num_images_per_prompt = 2 /n prior = StableCascadePriorPipeline.from_pretrained('stabilityai/stable-cascade-prior', torch_dtype=torch.bfloat16).to(device) /n decoder = StableCascadeDecoderPipeline.from_pretrained('stabilityai/stable-cascade',  torch_dtype=torch.float16).to(device) /n prompt = 'Anthropomorphic cat dressed as a pilot' /n negative_prompt = '' /n prior_output = prior( prompt=prompt,height=1024,width=1024,negative_prompt=negative_prompt,guidance_scale=4.0,num_images_per_prompt=num_images_per_prompt,num_inference_steps=20) /n decoder_output = decoder(image_embeddings=prior_output.image_embeddings.half(),prompt=prompt,negative_prompt=negative_prompt,guidance_scale=0.0,output_type='pil',num_inference_steps=10).images",
            "uses":"Research on generative models. /n Safe deployment of models which have the potential to generate harmful content. /n Probing and understanding the limitations and biases of generative models. /n Generation of artworks and use in design and other artistic processes. /n Applications in educational or creative tools."
        }
    }
}